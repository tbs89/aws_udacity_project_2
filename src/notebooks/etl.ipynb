{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7f48e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing copy: \n",
      "COPY staging_events\n",
      "FROM 's3://udacity-dend/log-data'\n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::891377030922:role/project-2-redshift-tbs89'\n",
      "REGION 'us-west-2'\n",
      "JSON 's3://udacity-dend/log_json_path.json'\n",
      "\n",
      "Executing copy: \n",
      "COPY staging_songs\n",
      "FROM 's3://udacity-dend/song-data'\n",
      "CREDENTIALS 'aws_iam_role=arn:aws:iam::891377030922:role/project-2-redshift-tbs89'\n",
      "REGION 'us-west-2'\n",
      "JSON 'auto'\n",
      "\n",
      "Executing insert: \n",
      "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
      "SELECT TIMESTAMP 'epoch' + e.ts / 1000 * INTERVAL '1 second' AS start_time,\n",
      "       e.userId AS user_id,\n",
      "       e.level,\n",
      "       s.song_id,\n",
      "       s.artist_id,\n",
      "       e.sessionId AS session_id,\n",
      "       e.location,\n",
      "       e.userAgent AS user_agent\n",
      "FROM staging_events e\n",
      "JOIN staging_songs s ON e.song = s.title AND e.artist = s.artist_name\n",
      "WHERE e.page = 'NextSong';\n",
      "\n",
      "Executing insert: \n",
      "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
      "SELECT DISTINCT userId AS user_id,\n",
      "                firstName AS first_name,\n",
      "                lastName AS last_name,\n",
      "                gender,\n",
      "                level\n",
      "FROM staging_events\n",
      "WHERE userId IS NOT NULL;\n",
      "\n",
      "Executing insert: \n",
      "INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
      "SELECT DISTINCT song_id,\n",
      "                title,\n",
      "                artist_id,\n",
      "                year,\n",
      "                duration\n",
      "FROM staging_songs;\n",
      "\n",
      "Executing insert: \n",
      "INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
      "SELECT DISTINCT artist_id,\n",
      "                artist_name AS name,\n",
      "                artist_location AS location,\n",
      "                artist_latitude AS latitude,\n",
      "                artist_longitude AS longitude\n",
      "FROM staging_songs;\n",
      "\n",
      "Executing insert: \n",
      "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
      "SELECT start_time,\n",
      "       EXTRACT(hour FROM start_time) AS hour,\n",
      "       EXTRACT(day FROM start_time) AS day,\n",
      "       EXTRACT(week FROM start_time) AS week,\n",
      "       EXTRACT(month FROM start_time) AS month,\n",
      "       EXTRACT(year FROM start_time) AS year,\n",
      "       EXTRACT(weekday FROM start_time) AS weekday\n",
      "FROM songplays;\n",
      "\n",
      "ETL process completed and connection closed.\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import copy_table_queries, insert_table_queries\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        print(\"Executing copy:\", query)\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during copy: {e}\")\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    for query in insert_table_queries:\n",
    "        print(\"Executing insert:\", query)\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during insert: {e}\")\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(\n",
    "            config.get(\"CLUSTER\", \"HOST\"),\n",
    "            config.get(\"CLUSTER\", \"DB_NAME\"),\n",
    "            config.get(\"CLUSTER\", \"DB_USER\"),\n",
    "            config.get(\"CLUSTER\", \"DB_PASSWORD\"),\n",
    "            config.get(\"CLUSTER\", \"DB_PORT\")))\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        load_staging_tables(cur, conn)\n",
    "        insert_tables(cur, conn)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Redshift: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print('ETL process completed and connection closed.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607af05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
